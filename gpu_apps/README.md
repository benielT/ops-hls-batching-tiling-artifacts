## GPU benchmark applications 

This contains GPU benchmark application codes, compiled artifacts and profiling data.

### Dependencies 

#### Hardware Requirements:
* Nvidia H100 

#### Runtime Software Requirements:
* Compatible NVIDIA Driver for CUDA 12.6
* Compatible NVIDIA-SMI for CUDA 12.6
* GNU Make (Any GNU Make compatible with GNU Make 4.3)
* OPS/OPS_batched (most of the GPU application build with [OPS_batched](../OPS_batched))

The OPS and OPS_batched are included as git submodule. For clone relevent submodule and setup instruction can be found in main [README.md](../README.md)

#### Build Software Requirements:

* python 3 > 3.8
* NVHPC 23.7 (compatible with other versions as well)
* HDF5 (optional)
* [OPS-DSL/tridsolver](https://github.com/OP-DSL/tridsolver)(optional)

#### Benchmarked System Specs:
* Processor           : AMD EPYC 9334 (32-Core) - 2x
* Memory              : 384GB RAM
* NVIDIA-SMI version  : 560.35.03
* NVML version        : 560.35
* DRIVER version      : 560.35.03
* CUDA Version        : 12.6
* OS                  : Linux/Debian 6 (amd_64)


### Getting started

The GPU applications are organized into seperate project containing it's own source code, prebuild binaries, etc following manner.

<pre> gpu_apps/ 
    ├── README.md 
    ├── app_1/h100_project/ 
    │   ├── profile_data/ 
    │   │   ├── runtime/
    │   │   └── power/
    │   ├── CUDA/ 
    │   │ ....
    │   ├── Makefile 
    │   └── run_script.sh 
    ├── app_2/h100_project/
    ...</pre>

Not all GPU apps are built with OPS_batched (which is a older variant of OPS for batched GPU mode) as [rtm app](./rtm_fp/h100_project/) is not running in batch mode and [blackschole app](./blackscholes/h100_project/) is implemented as 2D application to hold multiple batches. The details which app is built with which OPS version.

#### OPS based apps: 
* [blackscholes](./blackscholes/h100_project/) 
* [rtm_fp](./rtm_fp/h100_project/)

#### OPS_batched based apps:
* [heat3d](./heat3d/h100_project/)
* [jacobian2d](./jacobian2d/h100_project/)
* [jacobian3d](./jacobian3d/h100_project/)
* [laplace2d](./laplace2d/h100_project/)
* [poisson2d](./poisson2d/h100_project/)

The "profile_data" directory contains both <b>runtime</b> and <b>power</b> profile data and summaries catagorized by batch sizes. These data artifacts are directly used in our publication. 

Each projects contains prebuild binaries for H100 GPU with CUDA 12.6 part for NVHPC 13.7. The binary can be run with,

        make run_app

which will call "run_script.sh" inside each projects. <b>To run without building, only the [hardware requirements](#hardware-requirements) and [runtime software reqirements](#runtime-software-requirements) are adequate</b>.

<b> To run applications, first follow instructions in,</b>
* [Build OPS\OPS_batched](#build-opsops_batched) and then
* [Build + Run Apps](#build--run-apps)

### Pre-generated data artifacts

Each project contain our benchmarked profile data under ```profile_data``` with following structure:

<pre> gpu_app/h100_project/ 
    ├── profile_data/ 
    │   ├── runtime/ 
    │   │   ├── 1-batch/
    │   │   │   ├── 100_100_perf_profile.csv
    │   │   │   │  ....
    │   │   │   └── profile_summary.csv
    │   │   ├── 10-batch/
    │   │     ....
    │   └── power/ 
    │       ├── 1-batch/
    │       ├── 100-batch/
    │       │   ├── 100_100_nvidia_smi_log.csv
    │       │   │  ....
    │       │   └── power_profile_summary.csv
    │         ....
    ....</pre>

<b>runtime</b> contains profile runtimes for each grid dimensions and finally <b>profile_summary.csv</b> contains summerized runtimes for each grid size generated by [profile_summery_hls.py](../OPS/scripts/profile_summery_hls.py).

<b>power</b> contains power profile logs from nvidia smi API collected by support script [profile_with_nvidia_smi.sh](../scripts/profile_with_nvidia_smi.sh) conected with each ```run_script.sh``` inside projects. Additionally, contains <b>power_profile_summary.csv</b> contains estimated energy usage for a give batch size generated by [power_profile_summary_nvidia.py](../scripts/power_profile_summary_nvidia.py).

Further details in section [Run profile scripts for data summaries](#4-run-profile-scripts-for-data-summaries) how to run these support scripts to generate summaries.

### Build OPS\OPS_batched
#### 1. Setup environment

Sample setup script, [source_demos_nvhpc_23_7_ops.sh](../scripts/source_demos_nvhpc_23_7_ops.sh) is given. Please modify it accordingly before source. NOTE: Make sure environment variable OPS_HLS_ARTIFACT_DIR set as in main [README.md Getting Started - Step 2](../README.md#step-2-add-environment-variable).
        
        source $OPS_HLS_ARTIFACT_DIR/scripts/source_demos_nvhpc_23_7_ops.sh

NOTE: you can have your own environment setup script. But follow above script as guide. For further info about OPS setup check: [OPS/README.md](../OPS/README.md).

#### Setup OPS_batched environment

Similar to above [OPS setup](#1-setup-environment), a sample setup script [source_demos_nvhpc_23_7_ops_batch.sh](../scripts/source_demos_nvhpc_23_7_ops_batch.sh) is given. Follow similar as above, if you're building/running OPS_batched applications.

#### 2. Build

If proper setup completed as in step 1. Then, you'll be able to check ```echo $OPS_INSTALL_PATH``` which will point to [OPS/ops](../OPS/ops/) or [OPS_batched/ops](../OPS_batched/ops/). If <b>$OPS_INSTALL_PATH</b> is properly set, goto: ```cd $OPS_INSTALL_PATH/c``` and run ```make```.

Verify proper installation by check whether ```libops_cuda.a``` available inside ```OPS/ops/c/lib/pgi/``` or ```OPS_batched/ops/c/lib/pgi/```.

NOTE: Strongly recomment to build both OPS and OPS_batched both of them needed to verify all applications. You can easily switch between OPS or OPS_batched with setup scripts.

### Build + Run Apps

If the [Build OPS\OPS_batched](#build-opsops_batched) complated, you will be able to run prebuild binaries if your test environment is similar to our [benchmarked system](#benchmarked-system-specs). 

#### 1. Source relevent OPS version environment setup script

Make sure you source correct OPS version as in [Getting Started](#getting-started). For example, [poisson2d](./poisson2d/h100_project/) app requires OPS_batched. Therefore source OPS_batched source [source_demos_nvhpc_23_7_ops_batch.sh](../scripts/source_demos_nvhpc_23_7_ops_batch.sh) or your custom OPS_batched source file.

Try running app as in [step 3](#3-run-application). If this step not working it might be due to pre-build binary incompatibility (proceed to next section).

#### 2. Clean and build application

Run 

        make clean && make
        
for build the application from the source code. ```make clean``` will remove OPS generated code, objects and data artifacts from the projects. NOTE: you can use ```-j <number of threads>``` flag for make if need to build faster. 

#### 3. Run application

By defauit apps will have Makefile with ```CXXFLAGS += -DPROFILE```, which indicates runtime profiling enabled. With that run application with:

        make run_app

which will call ```run_script.sh``` inside the application project. You can add custom run paramenters in the run_script. If sucessfully ran, you'll have profile data inside ```profile_data/runtime/```. 

##### Power Profile

To swith to power profiling run, comment ```CXXFLAGS += -DPROFILE``` and uncomment ```CXXFLAGS += -DPOWER_PROFILE```. Similar to above use ```make run_app``` which will call ```run_script.sh``` inside the application project with recording profile data in ```profile_data/power/```. You can add custom run parameters in the run_script.


#### 4. Run profile scripts for data summaries

After running application with ```PROFILE``` flag you'll be having profile data inside ```profile_data/runtime``` with grid size and with mutiple records with the number of batch you have provided in the ```run_script.sh```. To get average runtime for these runs and summerize them into a sigle CSV, OPS contains support script, [profile_summery_hls.py](../OPS/scripts/profile_summery_hls.py).

first ```cd profile_data/runtime``` and then run profile script,

        python $OPS_HLS_ARTIFACT_DIR/OPS/scripts/profile_summery_hls.py -d .

NOTE: python3 environment with pandas required.

Similar to above, running application with ```POWER_PROFILE``` flag will produce profile data inside ```profile_data/power``` with grid size and with mutiple records with the number of batch you have provided in the ```run_script.sh```. To extract estimated energy useage and summerize them into a single CSV, use [power_profile_summary_nvidia.py](../scripts/power_profile_summary_nvidia.py) provided with this artifact. 

first ```cd profile_data/power``` and then run profile script,

        python $OPS_HLS_ARTIFACT_DIR/scripts/power_profile_summary_nvidia.py -d . -p <batch_size_need_to_be_estimated>

NOTE: ```<power_batch_size_need_to_be_estimated>``` is the required batch size you need for estimated energy usage. The pre genreated summary will have estimated batch sizes used in the publication. 


